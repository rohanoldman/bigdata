# create the 3 Java files in a folder ‘prac2’ while logged in as Hadoop user # WC_Mapper.java
package com.wordcountproblem;
import java.io.IOException; 
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.Reporter;
public class WC_Mapper extends MapReduceBase implements Mapper & lt;
LongWritable, Text, Text, IntWritable & gt; { 
private final static IntWritable one = new IntWritable(1); 
private Text word = new Text(); 
public void map(LongWritable key, Text value, OutputCollector & lt; Text, IntWritable & gt; output, 
Reporter reporter) throws IOException {
String line = value.toString();
StringTokenizer tokenizer = new StringTokenizer(line); 
while (tokenizer.hasMoreTokens()) { word.set(tokenizer.nextToken()); output.collect(word, one); 
} } 
} 
# WC_Reducer.java
package com.wordcountproblem;
import java.io.IOException;
import java.util.Iterator;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.MapReduceBase; import org.apache.hadoop.mapred.OutputCollector; import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.Reporter; 
public class WC_Reducer extends MapReduceBase implements Reducer & lt;
Text, IntWritable, Text, IntWritable & gt; { 
public void reduce(Text key, Iterator & lt; IntWritable & gt; values, OutputCollector & lt; Text, IntWritable & gt; output, 
Reporter reporter) throws IOException { int sum = 0;
while (values.hasNext()) { 
sum += values.next().get(); 
}
output.collect(key, new IntWritable(sum)); } 
} 
#WC_Runner
package com.wordcountproblem; 
import java.io.IOException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.FileInputFormat; import org.apache.hadoop.mapred.FileOutputFormat; import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat; import org.apache.hadoop.mapred.TextOutputFormat; public class WC_Runner { 
public static void main(String[] args) throws IOException { JobConf conf = new JobConf(WC_Runner.class); conf.setJobName( & quot; WordCount & quot;); conf.setOutputKeyClass(Text.class); conf.setOutputValueClass(IntWritable.class); conf.setMapperClass(WC_Mapper.class); conf.setCombinerClass(WC_Reducer.class); conf.setReducerClass(WC_Reducer.class); conf.setInputFormat(TextInputFormat.class); conf.setOutputFormat(TextOutputFormat.class); 
FileInputFormat.setInputPaths(conf, new Path(args[0])); FileOutputFormat.setOutputPath(conf, new Path(args[1])); JobClient.runJob(conf); 
} } 
# Compile the java files 
javac -classpath hadoop-core-1.2.1.jar -d wordcountproblem WC_Mapper.java WC.Reducer.java WC_Runner.java 
# Create the JAR file
$ jar -cvf wordcountproblem.jar -C wordcountproblem / 
# Create a text file in the same folder with some content
 $ sudo nano data.txt 
type any thing in the file similar 
# start the Hadoop server $ start-dfs.sh 
$ start-yarn.sh
# Uploading file to Hadoop 
    1. Go to http://localhost:9870/ > click on utilities > Browse the file system 
    2. Click on create a new folder and enter name as ‘test’ 
    3. Enter the created folder > click on upload button beside the create new folder button > 
       Locate the created text file and open it 
# Run the MapReduce program with the JAR file 
$ hadoop jar wordcountproblem.jar com.wordcountproblem.WC_Runner /test/data.txt /r_output 
# print the generated output to the console 
$ hdfs dfs -cat /r_output/part-00000 

